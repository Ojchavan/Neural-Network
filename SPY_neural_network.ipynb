{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "362b89a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from datetime import datetime, timedelta\n",
    "import requests\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb26eb2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SP500NeuralNetwork:\n",
    "    def __init__(self, lookback_days=30):\n",
    "        \"\"\"\n",
    "        Initialize the neural network for S&P 500 stocks prediction\n",
    "        \n",
    "        Parameters:\n",
    "        lookback_days (int): Number of previous days to use for prediction\n",
    "        \"\"\"\n",
    "        self.lookback_days = lookback_days\n",
    "        \n",
    "    def get_sp500_tickers(self):\n",
    "        \"\"\"Get current S&P 500 stock tickers\"\"\"\n",
    "        # Get S&P 500 tickers from Wikipedia\n",
    "        url = \"https://en.wikipedia.org/wiki/List_of_S%26P_500_companies\"\n",
    "        tables = pd.read_html(url)\n",
    "        sp500_table = tables[0]\n",
    "        return sp500_table['Symbol'].tolist()\n",
    "    \n",
    "    def get_stock_data(self, tickers):\n",
    "        \"\"\"\n",
    "        Fetch historical data for all S&P 500 stocks\n",
    "        \n",
    "        Parameters:\n",
    "        tickers (list): List of stock tickers\n",
    "        \"\"\"\n",
    "        end_date = datetime.now()\n",
    "        start_date = end_date - timedelta(days=5*365)  # 10 years of data\n",
    "        \n",
    "        all_stocks_data = {}\n",
    "        print(\"Fetching data for all S&P 500 stocks...\")\n",
    "        \n",
    "        for i, ticker in enumerate(tickers):\n",
    "            try:\n",
    "                stock = yf.Ticker(ticker)\n",
    "                data = stock.history(start=start_date, end=end_date)\n",
    "                if not data.empty:\n",
    "                    all_stocks_data[ticker] = data['Close']\n",
    "                if (i + 1) % 50 == 0:\n",
    "                    print(f\"Processed {i + 1} stocks\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error fetching data for {ticker}: {str(e)}\")\n",
    "                continue\n",
    "                \n",
    "        return pd.DataFrame(all_stocks_data)\n",
    "    \n",
    "    def prepare_data(self, stock_prices):\n",
    "        \"\"\"\n",
    "        Prepare data for neural network training\n",
    "        Returns sequences of lookback_days and their corresponding next day movements\n",
    "        \"\"\"\n",
    "        X, Y = [], []\n",
    "        \n",
    "        # Process each stock\n",
    "        for column in stock_prices.columns:\n",
    "            prices = stock_prices[column].dropna().values\n",
    "            if len(prices) <= self.lookback_days:\n",
    "                continue\n",
    "                \n",
    "            # Create sequences for this stock\n",
    "            for i in range(len(prices) - self.lookback_days):\n",
    "                sequence = prices[i:(i + self.lookback_days)]\n",
    "                next_price = prices[i + self.lookback_days]\n",
    "                \n",
    "                # Only add if we have valid data\n",
    "                if not np.any(np.isnan(sequence)) and not np.isnan(next_price):\n",
    "                    X.append(sequence)\n",
    "                    Y.append(1 if next_price > prices[i + self.lookback_days - 1] else 0)\n",
    "        \n",
    "        X = np.array(X)\n",
    "        Y = np.array(Y)\n",
    "        \n",
    "        if len(X) == 0:\n",
    "            raise ValueError(\"No valid sequences found in the data\")\n",
    "        \n",
    "        # Normalize each sequence\n",
    "        X = (X - np.mean(X, axis=1, keepdims=True)) / np.std(X, axis=1, keepdims=True)\n",
    "        \n",
    "        # Replace any remaining NaN values\n",
    "        X = np.nan_to_num(X)\n",
    "        \n",
    "        # Split into train and dev sets\n",
    "        split = int(0.8 * len(X))\n",
    "        X_train, X_dev = X[:split].T, X[split:].T\n",
    "        Y_train, Y_dev = Y[:split], Y[split:]\n",
    "        \n",
    "        return X_train, Y_train, X_dev, Y_dev\n",
    "    \n",
    "    def init_params(self):\n",
    "        \"\"\"Initialize neural network parameters\"\"\"\n",
    "        W1 = np.random.rand(20, self.lookback_days) - 0.5  # Increased hidden layer size\n",
    "        b1 = np.random.rand(20, 1) - 0.5\n",
    "        W2 = np.random.rand(2, 20) - 0.5\n",
    "        b2 = np.random.rand(2, 1) - 0.5\n",
    "        return W1, b1, W2, b2\n",
    "    \n",
    "    def ReLU(self, Z):\n",
    "        \"\"\"ReLU activation function\"\"\"\n",
    "        return np.maximum(Z, 0)\n",
    "    \n",
    "    def ReLU_deriv(self, Z):\n",
    "        \"\"\"Derivative of ReLU function\"\"\"\n",
    "        return Z > 0\n",
    "    \n",
    "    def softmax(self, Z):\n",
    "        \"\"\"Softmax activation function\"\"\"\n",
    "        exp = np.exp(Z - np.max(Z, axis=0, keepdims=True))\n",
    "        return exp / np.sum(exp, axis=0, keepdims=True)\n",
    "    \n",
    "    def forward_prop(self, W1, b1, W2, b2, X):\n",
    "        \"\"\"Forward propagation step\"\"\"\n",
    "        Z1 = W1.dot(X) + b1\n",
    "        A1 = self.ReLU(Z1)\n",
    "        Z2 = W2.dot(A1) + b2\n",
    "        A2 = self.softmax(Z2)\n",
    "        return Z1, A1, Z2, A2\n",
    "    \n",
    "    def one_hot(self, Y):\n",
    "        \"\"\"Convert labels to one-hot encoding\"\"\"\n",
    "        one_hot_Y = np.zeros((2, Y.size))\n",
    "        one_hot_Y[Y, np.arange(Y.size)] = 1\n",
    "        return one_hot_Y\n",
    "    \n",
    "    def backward_prop(self, Z1, A1, Z2, A2, W2, X, Y, m):\n",
    "        \"\"\"Backward propagation step\"\"\"\n",
    "        one_hot_Y = self.one_hot(Y)\n",
    "        dZ2 = A2 - one_hot_Y\n",
    "        dW2 = 1 / m * dZ2.dot(A1.T)\n",
    "        db2 = 1 / m * np.sum(dZ2, axis=1, keepdims=True)\n",
    "        dZ1 = W2.T.dot(dZ2) * self.ReLU_deriv(Z1)\n",
    "        dW1 = 1 / m * dZ1.dot(X.T)\n",
    "        db1 = 1 / m * np.sum(dZ1, axis=1, keepdims=True)\n",
    "        return dW1, db1, dW2, db2\n",
    "    \n",
    "    def update_params(self, W1, b1, W2, b2, dW1, db1, dW2, db2, alpha):\n",
    "        \"\"\"Update parameters using gradient descent\"\"\"\n",
    "        W1 = W1 - alpha * dW1\n",
    "        b1 = b1 - alpha * db1\n",
    "        W2 = W2 - alpha * dW2\n",
    "        b2 = b2 - alpha * db2\n",
    "        return W1, b1, W2, b2\n",
    "    \n",
    "    def get_predictions(self, A2):\n",
    "        \"\"\"Get predictions from network output\"\"\"\n",
    "        return np.argmax(A2, 0)\n",
    "    \n",
    "    def get_accuracy(self, predictions, Y):\n",
    "        \"\"\"Calculate prediction accuracy\"\"\"\n",
    "        return np.sum(predictions == Y) / Y.size\n",
    "    \n",
    "    def train(self, X, Y, alpha=0.01, iterations=10000):\n",
    "        \"\"\"Train the neural network\"\"\"\n",
    "        W1, b1, W2, b2 = self.init_params()\n",
    "        m = X.shape[1]\n",
    "        \n",
    "        for i in range(iterations):\n",
    "            Z1, A1, Z2, A2 = self.forward_prop(W1, b1, W2, b2, X)\n",
    "            dW1, db1, dW2, db2 = self.backward_prop(Z1, A1, Z2, A2, W2, X, Y, m)\n",
    "            W1, b1, W2, b2 = self.update_params(W1, b1, W2, b2, dW1, db1, dW2, db2, alpha)\n",
    "            \n",
    "            if i % 100 == 0:\n",
    "                predictions = self.get_predictions(A2)\n",
    "                accuracy = self.get_accuracy(predictions, Y)\n",
    "                print(f\"Iteration: {i}, Training Accuracy: {accuracy:.4f}\")\n",
    "                \n",
    "        return W1, b1, W2, b2\n",
    "    \n",
    "    def predict(self, X, W1, b1, W2, b2):\n",
    "        \"\"\"Make predictions using trained parameters\"\"\"\n",
    "        _, _, _, A2 = self.forward_prop(W1, b1, W2, b2, X)\n",
    "        return self.get_predictions(A2)\n",
    "\n",
    "def main():\n",
    "    # Initialize neural network\n",
    "    nn = SP500NeuralNetwork(lookback_days=30)\n",
    "    \n",
    "    # Get S&P 500 tickers\n",
    "    print(\"Fetching S&P 500 tickers...\")\n",
    "    tickers = nn.get_sp500_tickers()\n",
    "    \n",
    "    # Get historical data for all stocks\n",
    "    stock_prices = nn.get_stock_data(tickers)\n",
    "    \n",
    "    # Prepare data\n",
    "    print(\"\\nPreparing data...\")\n",
    "    X_train, Y_train, X_dev, Y_dev = nn.prepare_data(stock_prices)\n",
    "    \n",
    "    print(f\"\\nTraining data shape: {X_train.shape}\")\n",
    "    print(f\"Development data shape: {X_dev.shape}\")\n",
    "    \n",
    "    # Train the network\n",
    "    print(\"\\nTraining neural network...\")\n",
    "    W1, b1, W2, b2 = nn.train(X_train, Y_train)\n",
    "    \n",
    "    # Test on development set\n",
    "    dev_predictions = nn.predict(X_dev, W1, b1, W2, b2)\n",
    "    accuracy = nn.get_accuracy(dev_predictions, Y_dev)\n",
    "    print(f\"\\nDevelopment Set Accuracy: {accuracy:.4f}\")\n",
    "    \n",
    "    # Save the model parameters\n",
    "    np.savez('sp500_model.npz', W1=W1, b1=b1, W2=W2, b2=b2)\n",
    "    print(\"\\nModel parameters saved to 'sp500_model.npz'\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d1128e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Then run:\n",
    "# nn = SP500NeuralNetwork(lookback_days=30)\n",
    "# tickers = nn.get_sp500_tickers()\n",
    "# stock_prices = nn.get_stock_data(tickers)\n",
    "# X_train, Y_train, X_dev, Y_dev = nn.prepare_data(stock_prices)\n",
    "# W1, b1, W2, b2 = nn.train(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b36e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SP500NeuralNetwork:\n",
    "    def __init__(self, lookback_days=30):\n",
    "        \"\"\"Initialize the neural network for S&P 500 stock predictions.\"\"\"\n",
    "        self.lookback_days = lookback_days\n",
    "\n",
    "    def get_sp500_tickers(self):\n",
    "        \"\"\"Fetch the current list of S&P 500 tickers.\"\"\"\n",
    "        url = \"https://en.wikipedia.org/wiki/List_of_S%26P_500_companies\"\n",
    "        tables = pd.read_html(url)\n",
    "        sp500_table = tables[0]\n",
    "        return sp500_table['Symbol'].tolist()\n",
    "\n",
    "    def get_stock_data(self, tickers):\n",
    "        \"\"\"Fetch historical stock data for the given tickers.\"\"\"\n",
    "        end_date = datetime.now()\n",
    "        start_date = end_date - timedelta(days=5 * 365)\n",
    "        all_stocks_data = {}\n",
    "\n",
    "        print(\"Fetching stock data for all S&P 500 companies...\")\n",
    "        for i, ticker in enumerate(tickers):\n",
    "            try:\n",
    "                stock = yf.Ticker(ticker)\n",
    "                data = stock.history(start=start_date, end=end_date)\n",
    "                if not data.empty:\n",
    "                    all_stocks_data[ticker] = data['Close']\n",
    "                if (i + 1) % 50 == 0:\n",
    "                    print(f\"Processed {i + 1} stocks...\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error fetching data for {ticker}: {e}\")\n",
    "                continue\n",
    "        return pd.DataFrame(all_stocks_data)\n",
    "\n",
    "    def prepare_data(self, stock_prices):\n",
    "        \"\"\"Prepare data for neural network training.\"\"\"\n",
    "        X, Y = [], []\n",
    "\n",
    "        for column in stock_prices.columns:\n",
    "            prices = stock_prices[column].dropna().values\n",
    "            if len(prices) <= self.lookback_days:\n",
    "                continue\n",
    "            for i in range(len(prices) - self.lookback_days):\n",
    "                sequence = prices[i:(i + self.lookback_days)]\n",
    "                next_price = prices[i + self.lookback_days]\n",
    "                X.append(sequence)\n",
    "                Y.append(1 if next_price > prices[i + self.lookback_days - 1] else 0)\n",
    "\n",
    "        X = np.array(X)\n",
    "        Y = np.array(Y)\n",
    "\n",
    "        if len(X) == 0:\n",
    "            raise ValueError(\"No valid sequences found in the data\")\n",
    "\n",
    "        X = (X - np.mean(X, axis=1, keepdims=True)) / np.std(X, axis=1, keepdims=True)\n",
    "        X = np.nan_to_num(X)\n",
    "\n",
    "        split = int(0.8 * len(X))\n",
    "        X_train, X_dev = X[:split].T, X[split:].T\n",
    "        Y_train, Y_dev = Y[:split], Y[split:]\n",
    "        return X_train, Y_train, X_dev, Y_dev\n",
    "\n",
    "    def init_params(self, layer_sizes):\n",
    "        \"\"\"Initialize neural network parameters.\"\"\"\n",
    "        np.random.seed(42)\n",
    "        params = {}\n",
    "        for i in range(1, len(layer_sizes)):\n",
    "            params[f\"W{i}\"] = np.random.randn(layer_sizes[i], layer_sizes[i - 1]) * np.sqrt(2. / layer_sizes[i - 1])\n",
    "            params[f\"b{i}\"] = np.zeros((layer_sizes[i], 1))\n",
    "        return params\n",
    "\n",
    "    def ReLU(self, Z):\n",
    "        return np.maximum(Z, 0)\n",
    "\n",
    "    def ReLU_deriv(self, Z):\n",
    "        return Z > 0\n",
    "\n",
    "    def softmax(self, Z):\n",
    "        exp = np.exp(Z - np.max(Z, axis=0, keepdims=True))\n",
    "        return exp / np.sum(exp, axis=0, keepdims=True)\n",
    "\n",
    "    def forward_prop(self, X, params):\n",
    "        caches = {\"A0\": X}\n",
    "        L = len(params) // 2\n",
    "\n",
    "        for i in range(1, L):\n",
    "            Z = params[f\"W{i}\"].dot(caches[f\"A{i-1}\"]) + params[f\"b{i}\"]\n",
    "            A = self.ReLU(Z)\n",
    "            caches[f\"Z{i}\"] = Z\n",
    "            caches[f\"A{i}\"] = A\n",
    "\n",
    "        ZL = params[f\"W{L}\"].dot(caches[f\"A{L-1}\"]) + params[f\"b{L}\"]\n",
    "        AL = self.softmax(ZL)\n",
    "        caches[f\"Z{L}\"] = ZL\n",
    "        caches[f\"A{L}\"] = AL\n",
    "        return AL, caches\n",
    "\n",
    "    def backward_prop(self, Y, caches, params, reg_lambda):\n",
    "        m = Y.size\n",
    "        grads = {}\n",
    "        L = len(params) // 2\n",
    "        A_last = caches[f\"A{L}\"]\n",
    "        one_hot_Y = np.zeros((A_last.shape[0], m))\n",
    "        one_hot_Y[Y, np.arange(m)] = 1\n",
    "\n",
    "        dZL = A_last - one_hot_Y\n",
    "        grads[f\"dZ{L}\"] = dZL\n",
    "        grads[f\"dW{L}\"] = 1 / m * dZL.dot(caches[f\"A{L-1}\"].T) + (reg_lambda / m) * params[f\"W{L}\"]\n",
    "        grads[f\"db{L}\"] = 1 / m * np.sum(dZL, axis=1, keepdims=True)\n",
    "\n",
    "        for i in range(L - 1, 0, -1):\n",
    "            dZ = params[f\"W{i+1}\"].T.dot(grads[f\"dZ{i+1}\"]) * self.ReLU_deriv(caches[f\"Z{i}\"])\n",
    "            grads[f\"dZ{i}\"] = dZ\n",
    "            grads[f\"dW{i}\"] = 1 / m * dZ.dot(caches[f\"A{i-1}\"].T) + (reg_lambda / m) * params[f\"W{i}\"]\n",
    "            grads[f\"db{i}\"] = 1 / m * np.sum(dZ, axis=1, keepdims=True)\n",
    "\n",
    "        return grads\n",
    "\n",
    "    def update_params(self, params, grads, alpha):\n",
    "        for key in params.keys():\n",
    "            params[key] -= alpha * grads[f\"d{key}\"]\n",
    "        return params\n",
    "\n",
    "    def train(self, X, Y, layer_sizes, alpha=0.01, iterations=5000, reg_lambda=0.01):\n",
    "        params = self.init_params(layer_sizes)\n",
    "        m = X.shape[1]\n",
    "\n",
    "        for i in range(iterations):\n",
    "            AL, caches = self.forward_prop(X, params)\n",
    "            grads = self.backward_prop(Y, caches, params, reg_lambda)\n",
    "            params = self.update_params(params, grads, alpha)\n",
    "\n",
    "            if i % 100 == 0:\n",
    "                predictions = np.argmax(AL, axis=0)\n",
    "                accuracy = np.mean(predictions == Y)\n",
    "                print(f\"Iteration {i}, Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "        return params\n",
    "\n",
    "    def predict(self, X, params):\n",
    "        AL, _ = self.forward_prop(X, params)\n",
    "        return np.argmax(AL, axis=0)\n",
    "\n",
    "def main():\n",
    "    nn = SP500NeuralNetwork(lookback_days=30)\n",
    "    tickers = nn.get_sp500_tickers()\n",
    "    stock_prices = nn.get_stock_data(tickers)\n",
    "\n",
    "    print(\"\\nPreparing data...\")\n",
    "    X_train, Y_train, X_dev, Y_dev = nn.prepare_data(stock_prices)\n",
    "\n",
    "    layer_sizes = [X_train.shape[0], 128, 64, 32, 2]\n",
    "    print(\"\\nTraining neural network...\")\n",
    "    params = nn.train(X_train, Y_train, layer_sizes, alpha=0.001, iterations=5000, reg_lambda=0.01)\n",
    "\n",
    "    print(\"\\nEvaluating on development set...\")\n",
    "    dev_predictions = nn.predict(X_dev, params)\n",
    "    dev_accuracy = np.mean(dev_predictions == Y_dev)\n",
    "    print(f\"Development Set Accuracy: {dev_accuracy:.4f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e2df859",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4fcca5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed44840e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
